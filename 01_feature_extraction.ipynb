{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da99e183",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992df9b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Created dummy video at: D:\\AI Video detector\\dummy_video.mp4\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create a dummy video for testing\n",
    "dummy_video_path = \"D:\\AI Video detector\\dummy_video.mp4\"\n",
    "os.makedirs(os.path.dirname(dummy_video_path), exist_ok=True)\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(dummy_video_path, fourcc, 30.0, (256, 256))\n",
    "for i in range(150): \n",
    "    img = np.zeros((256, 256, 3), dtype=np.uint8)\n",
    "    cv2.putText(img, str(i), (100, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "    out.write(img)\n",
    "out.release()\n",
    "\n",
    "print(f\"Created dummy video at: {dummy_video_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9510f4c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component 1 (get_video_frames) defined.\n"
     ]
    }
   ],
   "source": [
    "def get_video_frames(video_path: str, num_frames: int = 24, window_sec: int = 2) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Loads a video, samples num_frames evenly over a window_sec duration,\n",
    "    and returns them as a pre-processed tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize((224, 224)), \n",
    "        transforms.ToTensor(),         \n",
    "        transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406], \n",
    "            std=[0.229, 0.224, 0.225]  \n",
    "        ),\n",
    "    ])\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        raise IOError(f\"Cannot open video file {video_path}\")\n",
    "\n",
    "\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    if fps == 0:\n",
    "        cap.release()\n",
    "        raise ValueError(f\"Video file {video_path} has 0 FPS.\")\n",
    "\n",
    "\n",
    "    frames_in_window = int(fps * window_sec)\n",
    "    end_frame_idx = min(frames_in_window, total_frames - 1)\n",
    "    \n",
    "\n",
    "    if end_frame_idx <= 0:\n",
    "        end_frame_idx = total_frames - 1\n",
    "\n",
    "    indices = np.linspace(0, end_frame_idx, num=num_frames, dtype=int)\n",
    "\n",
    "    frames = []\n",
    "    for idx in indices:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        if not ret:\n",
    "            if len(frames) > 0:\n",
    "                frames.append(frames[-1]) \n",
    "            else:\n",
    "                cap.release()\n",
    "                raise IOError(f\"Could not read any frames from {video_path}\")\n",
    "            continue\n",
    "\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame_pil = Image.fromarray(frame_rgb)\n",
    "        frame_tensor = preprocess(frame_pil)\n",
    "        frames.append(frame_tensor)\n",
    "\n",
    "    cap.release()\n",
    "    return torch.stack(frames)\n",
    "\n",
    "print(\"Component 1 (get_video_frames) defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21667da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component 2 (ReStraVFeatureExtractor) defined.\n"
     ]
    }
   ],
   "source": [
    "class ReStraVFeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dinov2 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14')\n",
    "        \n",
    "        self.dinov2.eval()\n",
    "        for param in self.dinov2.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Extracts features from the final block (block 11) of DINOv2\n",
    "        and flattens them.\n",
    "        \"\"\"\n",
    "        T = x.shape[0] \n",
    "        features_dict = self.dinov2.forward_features(x)\n",
    "        \n",
    "        cls_token = features_dict['x_norm_clstoken']       \n",
    "        patch_tokens = features_dict['x_norm_patchtokens'] \n",
    "        \n",
    "\n",
    "        cls_token = cls_token.unsqueeze(1)                \n",
    "        trajectory = torch.cat([cls_token, patch_tokens], dim=1) \n",
    "\n",
    "        return trajectory.flatten(start_dim=1)\n",
    "\n",
    "print(\"Component 2 (ReStraVFeatureExtractor) defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b3487d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component 3 (calculate_geometry) defined.\n"
     ]
    }
   ],
   "source": [
    "def calculate_geometry(trajectory: torch.Tensor) -> (torch.Tensor, torch.Tensor):\n",
    "    \"\"\"\n",
    "    Calculates the stepwise distance and curvature from the trajectory.\n",
    "    \"\"\"\n",
    "    \n",
    "    displacement_vectors = trajectory[1:] - trajectory[:-1] \n",
    "    \n",
    "    distances = torch.norm(displacement_vectors, p=2, dim=1) \n",
    "    \n",
    "    delta_z_i = displacement_vectors[:-1] \n",
    "    delta_z_ip1 = displacement_vectors[1:]\n",
    "\n",
    "    cosine_sim = F.cosine_similarity(delta_z_i, delta_z_ip1, dim=1)\n",
    "    \n",
    "    cosine_sim = torch.clamp(cosine_sim, -1.0, 1.0)\n",
    "    \n",
    "    curvature_rad = torch.acos(cosine_sim)\n",
    "    curvature_deg = curvature_rad * (180.0 / np.pi)\n",
    "    \n",
    "    return distances, curvature_deg\n",
    "\n",
    "print(\"Component 3 (calculate_geometry) defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93db7b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component 4 (aggregate_features) defined.\n"
     ]
    }
   ],
   "source": [
    "def aggregate_features(distances: torch.Tensor, curvatures: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Aggregates distance and curvature time-series into a single \n",
    "    21-dimensional feature vector.\n",
    "    \"\"\"\n",
    "    \n",
    "    d_early = distances[:7]\n",
    "    if len(d_early) < 7:\n",
    "        d_early = F.pad(d_early, (0, 7 - len(d_early)), 'constant', 0)\n",
    "        \n",
    "    c_early = curvatures[:6]\n",
    "    if len(c_early) < 6:\n",
    "        c_early = F.pad(c_early, (0, 6 - len(c_early)), 'constant', 0)\n",
    "\n",
    "    d_stats = torch.stack([\n",
    "        torch.mean(distances),\n",
    "        torch.var(distances, unbiased=True),\n",
    "        torch.min(distances),\n",
    "        torch.max(distances)\n",
    "    ]) if distances.numel() > 0 else torch.zeros(4)\n",
    "\n",
    "    c_stats = torch.stack([\n",
    "        torch.mean(curvatures),\n",
    "        torch.var(curvatures, unbiased=True),\n",
    "        torch.min(curvatures),\n",
    "        torch.max(curvatures)\n",
    "    ]) if curvatures.numel() > 0 else torch.zeros(4)\n",
    "        \n",
    "    feature_vector = torch.cat([d_early, c_early, d_stats, c_stats])\n",
    "    \n",
    "    return torch.nan_to_num(feature_vector, nan=0.0)\n",
    "\n",
    "print(\"Component 4 (aggregate_features) defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acfc2c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component 5 (ReStraVClassifier) defined.\n"
     ]
    }
   ],
   "source": [
    "class ReStraVClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=21, h1=64, h2=32):\n",
    "        \"\"\"\n",
    "        [cite_start]Defines the 2-layer MLP classifier as described in the paper[cite: 228].\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, h1),  \n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(h1, h2),         \n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(h2, 1)           \n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.network(x)\n",
    "\n",
    "print(\"Component 5 (ReStraVClassifier) defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fd2155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- üöÄ Starting Full Pipeline Test ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Randinu/.cache\\torch\\hub\\facebookresearch_dinov2_main\n",
      "C:\\Users\\Randinu/.cache\\torch\\hub\\facebookresearch_dinov2_main\\dinov2\\layers\\swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "C:\\Users\\Randinu/.cache\\torch\\hub\\facebookresearch_dinov2_main\\dinov2\\layers\\attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "C:\\Users\\Randinu/.cache\\torch\\hub\\facebookresearch_dinov2_main\\dinov2\\layers\\block.py:40: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models loaded successfully.\n",
      "C1 Output (Video Tensor):    torch.Size([24, 3, 224, 224])\n",
      "C2 Output (Trajectory 'Z'):  torch.Size([24, 98688])\n",
      "C3 Output (Dist/Curv):     torch.Size([23]), torch.Size([22])\n",
      "C4 Output (21D Vector 'y'):  torch.Size([21])\n",
      "\n",
      "--- ‚úÖ Test Complete ---\n",
      "Final Prediction (Untrained): Natural (0.0000)\n"
     ]
    }
   ],
   "source": [
    "print(\"--- üöÄ Starting Full Pipeline Test ---\")\n",
    "\n",
    "try:\n",
    "    feature_extractor = ReStraVFeatureExtractor().to(device)\n",
    "    classifier = ReStraVClassifier().to(device)\n",
    "    classifier.eval() \n",
    "    print(\"Models loaded successfully.\")\n",
    "\n",
    "    video_tensor = get_video_frames(dummy_video_path).to(device)\n",
    "    print(f\"C1 Output (Video Tensor):    {video_tensor.shape}\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        trajectory = feature_extractor(video_tensor)\n",
    "    print(f\"C2 Output (Trajectory 'Z'):  {trajectory.shape}\")\n",
    "    \n",
    "    distances, curvatures = calculate_geometry(trajectory.cpu())\n",
    "    print(f\"C3 Output (Dist/Curv):     {distances.shape}, {curvatures.shape}\")\n",
    "\n",
    "    final_feature_vector = aggregate_features(distances, curvatures).to(device)\n",
    "    print(f\"C4 Output (21D Vector 'y'):  {final_feature_vector.shape}\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logit = classifier(final_feature_vector)\n",
    "    \n",
    "    probability = torch.sigmoid(logit)\n",
    "    prediction = \"AI-Generated\" if probability.item() > 0.5 else \"Natural\"\n",
    "    \n",
    "    print(\"\\n--- ‚úÖ Test Complete ---\")\n",
    "    print(f\"Final Prediction (Untrained): {prediction} ({probability.item():.4f})\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n--- ‚ùå Test Failed ---\")\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043a6176",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from tqdm import tqdm \n",
    "\n",
    "base_data_path = \"D:\\\\AI Video detector\\\\data\\\\raw\"\n",
    "natural_dir = os.path.join(base_data_path, \"natural\")\n",
    "ai_dir = os.path.join(base_data_path, \"ai_generated\")\n",
    "\n",
    "output_dir = \"D:\\\\AI Video detector\\\\data\\\\processed\"\n",
    "output_csv = os.path.join(output_dir, \"features.csv\")\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "natural_files = glob.glob(os.path.join(natural_dir, \"*.*\"))\n",
    "ai_files = glob.glob(os.path.join(ai_dir, \"*.*\"))\n",
    "\n",
    "file_list = [(f, 0) for f in natural_files] + [(f, 1) for f in ai_files]\n",
    "print(f\"Found {len(natural_files)} natural videos.\")\n",
    "print(f\"Found {len(ai_files)} AI-generated videos.\")\n",
    "print(f\"Total videos to process: {len(file_list)}\")\n",
    "\n",
    "feature_extractor = ReStraVFeatureExtractor().to(device)\n",
    "\n",
    "results = []\n",
    "for file_path, label in tqdm(file_list, desc=\"Processing Videos\"):\n",
    "    try:\n",
    "        \n",
    "        video_tensor = get_video_frames(file_path).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            trajectory = feature_extractor(video_tensor)\n",
    "\n",
    "        distances, curvatures = calculate_geometry(trajectory.cpu())\n",
    "\n",
    "        final_feature_vector = aggregate_features(distances, curvatures)\n",
    "\n",
    "        results.append({\n",
    "            \"file\": os.path.basename(file_path),\n",
    "            \"label\": label,\n",
    "            \"features\": final_feature_vector.numpy() \n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Failed to process {file_path}. Error: {e}\")\n",
    "        print(\"Skipping this file.\")\n",
    "\n",
    "if results:\n",
    "\n",
    "    feature_names = [f\"f_{i}\" for i in range(21)]\n",
    "    \n",
    "    df_records = []\n",
    "    for res in results:\n",
    "        record = {\n",
    "            \"file\": res[\"file\"],\n",
    "            \"label\": res[\"label\"]\n",
    "        }\n",
    "        for i, val in enumerate(res[\"features\"]):\n",
    "            record[feature_names[i]] = val\n",
    "        df_records.append(record)\n",
    "\n",
    "    df = pd.DataFrame(df_records)\n",
    "    \n",
    "    df.to_csv(output_csv, index=False)\n",
    "    \n",
    "    print(\"\\n--- üöÄ Processing Complete! ---\")\n",
    "    print(f\"Successfully processed {len(df)} videos.\")\n",
    "    print(f\"Features saved to: {output_csv}\")\n",
    "    \n",
    "    print(\"\\n--- Data Head: ---\")\n",
    "    print(df.head())\n",
    "    \n",
    "else:\n",
    "    print(\"\\n--- ‚ùå No results processed. Check your data paths. ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5caec4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- üöÄ CUDA is Available! ---\n",
      "Device Name: NVIDIA GeForce RTX 3050 Ti Laptop GPU\n",
      "Number of GPUs: 1\n",
      "Current Device Index: 0\n",
      "\n",
      "Testing tensor allocation...\n",
      "Successfully allocated tensor on GPU: tensor([1., 2.], device='cuda:0')\n",
      "Your environment is ready for GPU training.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"--- üöÄ CUDA is Available! ---\")\n",
    "    print(f\"Device Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current Device Index: {torch.cuda.current_device()}\")\n",
    "    \n",
    "    # We can also create a tensor and send it to the GPU\n",
    "    print(\"\\nTesting tensor allocation...\")\n",
    "    try:\n",
    "        x = torch.tensor([1.0, 2.0]).to(\"cuda\")\n",
    "        print(f\"Successfully allocated tensor on GPU: {x}\")\n",
    "        print(\"Your environment is ready for GPU training.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n--- ‚ùå Error allocating tensor to GPU ---\")\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n--- ‚ùå CUDA is NOT Available ---\")\n",
    "    print(\"PyTorch cannot detect your GPU.\")\n",
    "    print(\"Training will run on the CPU (which will be much slower).\")\n",
    "    print(\"Please check your PyTorch installation and CUDA driver versions.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
